{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import logging\n",
    "\n",
    "u_wind = ('src/data/prediction_data/eastward_near_surface_wind-ssp2_4_5_2016_2046.nc')\n",
    "ds = xr.open_dataset(u_wind)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction_data table created successfully.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import logging\n",
    "\n",
    "#Set up logging\n",
    "logging.basicConfig(filename='app.log', filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#TODO take out var name, make loop to get file paths. Remove variable from loop below\n",
    "#List of NetCDF files with predicted climate data\n",
    "files = [('src/data/prediction_data/eastward_near_surface_wind-ssp2_4_5_2016_2046.nc', 'u_wind'),\n",
    "         ('src/data/prediction_data/northward_near_surface_wind-ssp2_4_5_2016_2046.nc', 'v_wind'),\n",
    "         ('src/data/prediction_data/precipitation-ssp2_4_5_2016_2046.nc', 'precipitation'),\n",
    "         ('src/data/prediction_data/near_surface_relative_humidity-ssp2_4_5_2016_2046.nc', 'relative_humidity'),\n",
    "         ('src/data/prediction_data/near_surface_air_temperature-ssp2_4_5_2016_2046.nc', 'temperature')\n",
    "         ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to append the single datasets to\n",
    "ds_list = []\n",
    "\n",
    "# loop trough the files and load the datasets \n",
    "for file, variable in files:\n",
    "    #try except block needed for log file\n",
    "    try:\n",
    "        # Load NetCDF file\n",
    "        ds = xr.open_dataset(file)\n",
    "\n",
    "        #append the arrays to the list\n",
    "        ds_list.append(ds)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error('Error opening dataset for variable %s: %s', e)\n",
    "\n",
    "# merge all datasets into one, things like lat, lon and time, that are the same for all, are merged, variable values are kept. \n",
    "# compat = override needed for height, a dimension present only in some datasets. Ignores it if it is not present \n",
    "merged_ds = xr.merge(ds, compat='override')\n",
    "\n",
    "\n",
    "# drop unneeded bnds dimension and all variables it includes\n",
    "ds_dropped = merged_ds.drop_dims('bnds')\n",
    "ds_dropped = ds_dropped.reset_coords(drop = True)\n",
    "\n",
    "# convert xr dataset to pd dataframe\n",
    "dataframe = ds_dropped.to_dataframe().reset_index()\n",
    "\n",
    "# Adjust longitudes to have the same format as the past dataset\n",
    "dataframe['lon'] = dataframe['lon'].apply(lambda x: x - 360 if x > 180 else x)\n",
    "\n",
    "# Store the prediction dataframe in SQLite table \n",
    "with sqlite3.connect('test_data.db') as conn:\n",
    "    dataframe.to_sql('prediction_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "print('Prediction_data table created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 2022-12-02T00:00:00.000000000\n",
      "tcc 2022-12-01T23:00:00.000000000\n",
      "2t 2022-12-01T23:00:00.000000000\n",
      "2d 2022-12-01T23:00:00.000000000\n",
      "10v 2022-12-01T23:00:00.000000000\n",
      "10u 2022-12-01T23:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "filename = \"src/data/past_climate.grib\"\n",
    "variables = ['tp', 'tcc',  '2t','2d','10v', '10u'] \n",
    "past_ds_list = []\n",
    "\n",
    "for var in variables:\n",
    "    if var == 'tp':\n",
    "        ds_tp = xr.open_dataset(filename, engine='cfgrib', backend_kwargs={'filter_by_keys': {'shortName': var}})\n",
    "        \n",
    "        #standardize tp to make it in the same format as the other variables \n",
    "        # Flatten the 'valid_time' coordinate from 2D to 1D\n",
    "        ds_tp['valid_time'] = ds_tp['valid_time'].values.flatten()\n",
    "        \n",
    "        # Stack the 'time' and 'step' dimensions of the 'tp' variable into a single new dimension called 'total_time'\n",
    "        ds_tp['tp'] = ds_tp['tp'].stack(total_time=('time', 'step'))\n",
    "        \n",
    "        # Assign the flattened 'valid_time' data to the 'total_time' coordinate\n",
    "        ds_tp.coords[\"valid_time\"] = (\"total_time\", ds_tp['valid_time'].data)\n",
    "        \n",
    "        # Swap the 'total_time' dimension with the 'valid_time' dimension\n",
    "        ds_tp['tp'] = ds_tp['tp'].swap_dims({'total_time': 'valid_time'})\n",
    "        \n",
    "        # Drop the 'time', 'step', 'number', and 'surface' dimensions/variables\n",
    "        ds_tp_dropped = ds_tp.drop_dims(['time', 'step'])\n",
    "        ds_tp_dropped = ds_tp_dropped.drop_vars(['number', 'surface'])\n",
    "        \n",
    "        # Rename the 'valid_time' dimension to 'time' in 'tp_dropped' and assign the result back to 'datasets['tp']'\n",
    "        ds_tp_dropped = ds_tp_dropped.rename({'valid_time' : 'time'})\n",
    "        \n",
    "        # Drop all NA values in the 'time' dimension of 'datasets['tp']' and assign the result back to 'datasets['tp']'\n",
    "        ds_tp_dropped = ds_tp_dropped.dropna(dim='time', how='all')\n",
    "        \n",
    "        # Reorder the dimensions of 'datasets['tp']' to have 'time' as the first dimension\n",
    "        ds_tp_dropped = ds_tp_dropped.transpose('time', 'latitude', 'longitude')\n",
    "        \n",
    "        #eppend to dataset list\n",
    "        past_ds_list.append(ds_tp_dropped)     \n",
    "        print('tp:', ds_tp_dropped['time'].values[-1])  \n",
    "        \n",
    "    else:\n",
    "        #open the rest of the datasets and append them to the list\n",
    "        ds_past = xr.open_dataset(filename, engine='cfgrib', backend_kwargs={'filter_by_keys': {'shortName': var}})\n",
    "        past_ds_list.append(ds_past)\n",
    "        print(var, ds_past['time'].values[-1])\n",
    "        \n",
    "#merged_ds_past = xr.merge(past_ds_list)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'tp' (time: 9300, latitude: 9, longitude: 9)>\n",
      "array([[[           nan,            nan,            nan, ...,\n",
      "                    nan,            nan,            nan],\n",
      "        [           nan,            nan,            nan, ...,\n",
      "                    nan,            nan,            nan],\n",
      "        [           nan,            nan,            nan, ...,\n",
      "                    nan,            nan,            nan],\n",
      "        ...,\n",
      "        [           nan,            nan,            nan, ...,\n",
      "                    nan,            nan,            nan],\n",
      "        [           nan,            nan,            nan, ...,\n",
      "                    nan,            nan,            nan],\n",
      "        [           nan,            nan,            nan, ...,\n",
      "                    nan,            nan,            nan]],\n",
      "\n",
      "       [[8.07924607e-06, 5.18469551e-06, 5.19587138e-06, ...,\n",
      "         7.82592633e-06, 6.57050350e-06, 8.61196258e-06],\n",
      "        [1.25421438e-05, 8.79450181e-06, 8.03826788e-06, ...,\n",
      "         1.18902180e-05, 1.04038272e-05, 1.50567148e-05],\n",
      "        [1.10259507e-05, 7.79239872e-06, 7.08831885e-06, ...,\n",
      "         2.25259209e-05, 1.22701977e-05, 1.27805624e-05],\n",
      "...\n",
      "        [1.46134786e-04, 9.47928347e-05, 6.75535121e-05, ...,\n",
      "         1.59954907e-05, 1.23298059e-05, 1.78953887e-05],\n",
      "        [2.57148437e-04, 2.27092794e-04, 1.26688770e-04, ...,\n",
      "         2.43997456e-05, 2.54800798e-05, 2.93022276e-05],\n",
      "        [3.08423332e-04, 2.16088287e-04, 1.20601646e-04, ...,\n",
      "         3.57469798e-05, 4.76679088e-05, 5.10132195e-05]],\n",
      "\n",
      "       [[8.75930855e-05, 1.03272832e-04, 1.00296325e-04, ...,\n",
      "         4.61969466e-06, 4.22853918e-06, 4.51166125e-06],\n",
      "        [1.24167986e-04, 1.26172192e-04, 1.27818770e-04, ...,\n",
      "         3.54308577e-06, 2.41804810e-06, 4.42225428e-06],\n",
      "        [1.59815288e-04, 1.61927528e-04, 1.62519849e-04, ...,\n",
      "         5.95334859e-06, 4.70537634e-06, 9.64138599e-06],\n",
      "        ...,\n",
      "        [1.34393908e-04, 1.11528076e-04, 9.85417137e-05, ...,\n",
      "         2.73253390e-05, 1.54528389e-05, 1.47077808e-05],\n",
      "        [2.04753465e-04, 1.98554582e-04, 1.54443420e-04, ...,\n",
      "         3.32597265e-05, 2.41290400e-05, 2.49299774e-05],\n",
      "        [2.22284682e-04, 1.89688391e-04, 1.11144371e-04, ...,\n",
      "         3.43586871e-05, 4.98894224e-05, 5.07648656e-05]]], dtype=float32)\n",
      "Coordinates:\n",
      "  * latitude    (latitude) float64 38.96 38.71 38.46 38.21 ... 37.46 37.21 36.96\n",
      "  * longitude   (longitude) float64 -3.435 -3.185 -2.935 ... -1.934 -1.684 358.6\n",
      "  * time        (time) datetime64[ns] 1992-01-01 ... 2022-12-02\n",
      "    number      int32 0\n",
      "    step        timedelta64[ns] 00:00:00\n",
      "    surface     float64 0.0\n",
      "    valid_time  (time) datetime64[ns] 1992-01-01 1992-01-01T01:00:00 ... NaT\n",
      "Attributes: (12/30)\n",
      "    GRIB_paramId:                             228\n",
      "    GRIB_dataType:                            fc\n",
      "    GRIB_numberOfPoints:                      81\n",
      "    GRIB_typeOfLevel:                         surface\n",
      "    GRIB_stepUnits:                           1\n",
      "    GRIB_stepType:                            avgas\n",
      "    ...                                       ...\n",
      "    GRIB_shortName:                           tp\n",
      "    GRIB_totalNumber:                         0\n",
      "    GRIB_units:                               m\n",
      "    long_name:                                Total precipitation\n",
      "    units:                                    m\n",
      "    standard_name:                            unknown\n"
     ]
    }
   ],
   "source": [
    "print(merged_ds_past['tp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ds_past = xr.open_dataset(filename, engine='cfgrib', backend_kwargs={'filter_by_keys': {'shortName': var}})\n",
    "    past_ds_list.append(ds_past)\n",
    "\n",
    "merged_ds_past = xr.merge(past_ds_list)\n",
    "\n",
    "merged_ds_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a SQLite database    \n",
    "   print('Prediction_data table created successfully.')\n",
    "   # Open the GRIB file with past data\n",
    "   print('opening GRIB file')\n",
    "   filename = \"src/data/past_climate.grib\"\n",
    "   variables = ['tp', 'tcc', 'rh', '2t','2d','10v', '10u'] \n",
    "   datasets = {}\n",
    "   # Set up Dask to use a single thread\n",
    "   dask.config.set(scheduler='single-threaded')\n",
    "   for var in variables:\n",
    "       try:\n",
    "           # Open the GRIB file with chunks\n",
    "           ds = xr.open_dataset(filename, engine='cfgrib', backend_kwargs={'filter_by_keys': {'shortName': var}}, chunks={'time': 10})\n",
    "           datasets[var] = ds\n",
    "       except Exception as e:\n",
    "           logging.error('Error opening dataset for variable %s: %s', var, e)\n",
    "   #calculate relative humidity from temperature and dewpoint temperature\n",
    "   def rh(dewpoint, temperature):\n",
    "       return 100*(np.exp((17.625*dewpoint)/(243.04+dewpoint))/np.exp((17.625*temperature)/(243.04+temperature)))\n",
    "   rh_all = rh(datasets['2d']['d2m']-273.15, datasets['2t']['t2m']-273.15)\n",
    "   datasets['rh'] = xr.Dataset({'rh': xr.DataArray(rh_all, coords=datasets['2d']['d2m'].coords, dims=datasets['2d']['d2m'].dims)})\n",
    "   #standardize tp to make it in the same format as the other variables \n",
    "   # Flatten the 'valid_time' coordinate from 2D to 1D\n",
    "   datasets['tp']['valid_time'] = datasets['tp']['valid_time'].values.flatten()\n",
    "   # Stack the 'time' and 'step' dimensions of the 'tp' variable into a single new dimension called 'total_time'\n",
    "   datasets['tp']['tp'] = datasets['tp']['tp'].stack(total_time=('time', 'step'))\n",
    "   # Assign the flattened 'valid_time' data to the 'total_time' coordinate\n",
    "   datasets['tp'].coords[\"valid_time\"] = (\"total_time\", datasets['tp']['valid_time'].data)\n",
    "   # Swap the 'total_time' dimension with the 'valid_time' dimension\n",
    "   datasets['tp'] = datasets['tp'].swap_dims({'total_time': 'valid_time'})\n",
    "   # Drop the 'time', 'step', 'number', and 'surface' dimensions from 'tp_single_time_and_step' and assign the result to 'tp_dropped'\n",
    "   tp_dropped = datasets['tp'].drop(['time','step','number','surface'])\n",
    "   # Rename the 'valid_time' dimension to 'time' in 'tp_dropped' and assign the result back to 'datasets['tp']'\n",
    "   datasets['tp'] = tp_dropped.rename({'valid_time': 'time'})\n",
    "   # Drop all NA values in the 'time' dimension of 'datasets['tp']' and assign the result back to 'datasets['tp']'\n",
    "   datasets['tp'] = datasets['tp'].dropna(dim='time', how='all')\n",
    "   #TODO:check if the transpose works correctly\n",
    "   # Reorder the dimensions of 'datasets['tp']' to have 'time' as the first dimension\n",
    "   datasets['tp'] = datasets['tp'].transpose('time', 'latitude', 'longitude')\n",
    "   print('writing to database')\n",
    "   for var, ds in datasets.items():\n",
    "       try:\n",
    "           print('processing', var)\n",
    "           with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "           # Convert the Dataset to a DataFrame and reset the index\n",
    "               df = ds.to_dask_dataframe().reset_index()\n",
    "           #print(df.dtypes)\n",
    "           #print(len(df))\n",
    "           # Convert the DataFrame to a Dask DataFrame\n",
    "           #df = dd.from_pandas(df, npartitions=4)  \n",
    "           df['variable'] = var\n",
    "           print('writing', var, 'to database')\n",
    "           # Write the DataFrame to the SQLite database in chunks\n",
    "           # Use the variable name as the table name\n",
    "           # Write the DataFrame to the SQLite database in chunks\n",
    "           # Use the variable name as the table name\n",
    "           print('Number of partitions:', df.npartitions)\n",
    "           print('Size of DataFrame:', len(df))\n",
    "           for i in range(df.npartitions):\n",
    "               # Compute the partition to load the data into memory and convert it to a Pandas DataFrame\n",
    "               partition_pd = df.get_partition(i).compute()\n",
    "               #print(type(partition_pd))\n",
    "               #print(type(partition_pd.empty)) \n",
    "                   \n",
    "               # Convert timedelta columns to strings\n",
    "               for col in partition_pd.columns:\n",
    "                   if partition_pd[col].dtype == 'timedelta64[ns]':\n",
    "                       partition_pd[col] = partition_pd[col].astype(str)\n",
    "               # Check if the DataFrame is empty\n",
    "               if not partition_pd.empty:\n",
    "                   # Write the DataFrame to the database\n",
    "                   partition_pd.to_sql(var + \"_climate\", conn, if_exists='append', index=False)\n",
    "                   # Commit the transaction to ensure the data is saved to the database\n",
    "                   conn.commit()\n",
    "       except Exception as e:\n",
    "           logging.error('Error writing variable %s to database: %s', var, e)\n",
    "print('Database created successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_permaculture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
